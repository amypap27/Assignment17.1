# Assignment17.1
This is CBMLAI Assignment 17.1: Practical Application 3

[Link to Jupyter Notebook](https://github.com/amypap27/Assignment17.1/blob/main/prompt_III.ipynb)

Summary of Findings
The goal of this project was to develop a machine-learning model that would effectively predict whether a marketing call to a customer would result in the client subscribing to a term deposit. Since this is a Yes/No classification task, I tried the classification models we have studied in this class, specifically: Logistic Regression, K-Nearest Neighbors, Decision Trees, and SVM.

In the first part of the project, I explored the dataset, which was provided for us and is from a Portuguese banking institution. I read through the description of the various columns, ran some statistics on the numeric and categorical columns, as well as created a number of visualizations to understand the data better.

This dataset was much cleaner than the one from Assignment 11.1 - Pratical Application 2.  There were not a lot of missing values that needed to be dealt with this time.  However, the big issue this dataset was facing, is that it is very imbalanced. More specifically, only about 11% of the rows are "Yes" and the remaining almost 89% are "No" in the target column. This definitely presents a challenge for building an effective model. In fact, my initial Dummy model had an accuracy of almost 89% because it just guesses "No" for all rows!

To prepare for building models, I used StandardScaler to scale the data, used One-Hot Encoding for all categorical columns, and then split the dataset into training and test datasets. As per the column descriptions and related article, I chose to drop the duration column, as this column is highly correlated with the target column but doesn't make sense for a predictive model, as it is information that would not be available in advance.  I also chose to drop the economic indices and focus on the client data.

I built initial LogisticRegression, KNN, DecisionTree and SVM models using the default parameters. Given the imbalance of the dataset target column, I realized accuracy wasn't a great metric. I therefore also looked at recall, which measures the percengage of "Yes" values that we correctly predict, out of the total "Yes" values.  For this business problem, this seemed particularly important, because for this banking business to be successful, we want to be sure to decide to call as many clients that will say Yes as possible.  Otherwise we are missing opportunities to do more client business.  While it is unfortunate if we waste time calling a lot of clients that will say no, this is a smaller issue than missing a lot of "Yes" clients.

When I compared the new models to the Dummy model, most had slightly better accuracy scores on training and test data. The one exception was the Decision Tree model which had much better accuracy (99%) on the training data but was worse (83%) on the test data.  I suspect this was because the model was unconstrained and therefore probably overfit to the training data.  However, all of these models were better on recall than the Dummy Model (which had 0% recall). From a recall perspective, the Decision Tree did the best (94% on training but only 30% on the test data). The other models were a bit lower but had less of a differece between training and test data results, suggesting less overfitting.

I then attempted to use GridSearchCV to try to find optimal hyperparameters for these models. This worked reasonably well for Logistic Regression, KNN and Decision Trees.  However, the SVM models did not work well with GridSearchCV because they took too long to complete.  So instead I manually explored some different SVM options, finding that both polynomial and RBF models were a bit more successful on recall than linear ones.  I also found that increasing the C value improved these models a bit.

Overall, all of the classification models were better from a recall perspective than the Dummy model. But none of them had a very high recall score, meaning that they would all miss recommending many of the marketing calls that led to "Yes" results. As a result, I would recommend some further tuning, exploration of better models, or perhaps gathering some additional data if possible that might lead to improvements.
